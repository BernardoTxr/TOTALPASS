{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POLI JÚNIOR + TOTAL PASS\n",
        "### REGRESSOR PARA CHURN RATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importando bibliotecas necessárias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EyJsPrZji-g2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import seaborn as sns\n",
        "from random import randint\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.stats import randint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from futures3.thread import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from pymannkendall import original_test\n",
        "import pytz\n",
        "from pyarrow.parquet import ParquetFile\n",
        "import pyarrow as pa\n",
        "import shap\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# Evitar warnings:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9iWiaTbhi-gz"
      },
      "source": [
        "### Parâmetros do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C1yMwM0Mi-g4"
      },
      "outputs": [],
      "source": [
        "DATE_OF_INFERENCE = '2023-11-17' # data de hoje\n",
        "CUT_OFF_DATE = '2022-11-17' # 1 ano atrás\n",
        "\n",
        "# Tabelas dinâmicas Total Pass\n",
        "daily_token_filepath = 'totalpass/Tabelas/daily_token_validation_parquet.parquet'\n",
        "employees_filepath = 'totalpass/Tabelas/employees.csv'\n",
        "gyms_filepath = 'totalpass/Tabelas/gyms.csv'\n",
        "freezes_filepath = 'totalpass/Tabelas/freezes.csv'\n",
        "subscriptions_filepath = 'totalpass/Tabelas/subscriptions.csv'\n",
        "subscriptions_features_dataset_filepath = 'totalpass/Tabelas/subscriptions_features_dataset.csv'\n",
        "\n",
        "# Tabelas fixas Poli Júnior\n",
        "features_filepath = 'totalpass/Tabelas/features.csv'\n",
        "tendencia_de_utilizacao_filepath = 'totalpass/Tabelas/tendência_de_utilização.csv'\n",
        "cluster_filepath = 'totalpass/Tabelas/df_cluster.csv'\n",
        "dt_interval = [90, 180, 270, 365]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YzTXVMAti-g5"
      },
      "outputs": [],
      "source": [
        "def criar_features(df_token,df_gyms,df_subscriptions):\n",
        "    df1 = df_token[['validated_at','employee_id','token','gym_id']]\n",
        "    df2 = df_token[['employee_id','gym_id']]\n",
        "    df3 = df_token[['employee_id','validated_at_minute']]\n",
        "    academias_mais_utilizadas = df1.groupby(['employee_id', 'gym_id']).size().reset_index(name='utilizacoes')\n",
        "    idx = academias_mais_utilizadas.groupby(['employee_id'])['utilizacoes'].transform(max) == academias_mais_utilizadas['utilizacoes']\n",
        "    academias_mais_utilizadas = academias_mais_utilizadas[idx]\n",
        "\n",
        "    gyms1 = df_gyms[['gym_id','gym_status']]\n",
        "    gyms2 = df_gyms[['gym_id','main_modality']]\n",
        "    gymsdf = pd.merge(gyms2, df2, how= 'right', on= ['gym_id'])\n",
        "    dfprinc = pd.merge(gyms1, academias_mais_utilizadas, on='gym_id', how='right')\n",
        "\n",
        "    modalidades_preferidas = gymsdf.groupby(['employee_id', 'main_modality']).size().reset_index(name='num_main_modality')\n",
        "    idx = modalidades_preferidas.groupby(['employee_id'])['num_main_modality'].transform(max) == modalidades_preferidas['num_main_modality']\n",
        "    modalidades_preferidas = modalidades_preferidas[idx]\n",
        "    modalidades_preferidas = modalidades_preferidas.rename(columns={'main_modality': 'pref_modality'})\n",
        "    gymsprinc = pd.merge(gymsdf, modalidades_preferidas[['employee_id', 'pref_modality', 'num_main_modality']],\n",
        "              on=['employee_id'], how='left')\n",
        "\n",
        "    dfprinc = pd.merge(dfprinc, gymsprinc[['employee_id','pref_modality','num_main_modality']], how= 'left', on= ['employee_id'])\n",
        "    dfprinc = dfprinc.drop_duplicates()\n",
        "    academias_distintas_por_usuario = df2.groupby('employee_id')['gym_id'].nunique().reset_index(name='num_distinct_gyms')\n",
        "    distpref = pd.merge(df2, academias_distintas_por_usuario, on='employee_id', how='left')\n",
        "\n",
        "    dfprinc = pd.merge(dfprinc, distpref[['employee_id','num_distinct_gyms']], how= 'left', on= ['employee_id'])\n",
        "    dfprinc = dfprinc.drop_duplicates()\n",
        "    subs1 = df_subscriptions[['employee_id','payment_source']]\n",
        "\n",
        "    dfprinc = pd.merge(dfprinc, subs1, how= 'left', on= ['employee_id'])\n",
        "    dfprinc = dfprinc.drop_duplicates()\n",
        "    df3['validated_at_minute'] = pd.to_datetime(df3['validated_at_minute'])\n",
        "    hora_mais_frequente_por_usuario = df3.groupby('employee_id')['validated_at_minute'].agg(lambda x: x.dt.hour.mode().iloc[0])\n",
        "    df3 = pd.merge(df3, hora_mais_frequente_por_usuario, on='employee_id', how='left')\n",
        "    df3 = df3.rename(columns={'validated_at_minute_y': 'most_frequent_hour'})\n",
        "\n",
        "    dfprinc = pd.merge(dfprinc, df3[['employee_id','most_frequent_hour']], how= 'left', on= ['employee_id'])\n",
        "    dfprinc = dfprinc.drop_duplicates()\n",
        "    df3['Ano'] = df3['validated_at_minute_x'].dt.year\n",
        "    df3['Semana'] = df3['validated_at_minute_x'].dt.isocalendar().week\n",
        "    num_utilizacoes_por_semana = df3.groupby(['employee_id', 'Ano', 'Semana']).size().reset_index(name='Num_Utilizacoes')\n",
        "    media_utilizacoes_por_semana = num_utilizacoes_por_semana.groupby('employee_id')['Num_Utilizacoes'].mean().reset_index(name='uses_per_week')\n",
        "\n",
        "    dfprinc = pd.merge(dfprinc, media_utilizacoes_por_semana, how= 'left', on= ['employee_id'])\n",
        "\n",
        "    return dfprinc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IUIWNyTAi-g6"
      },
      "outputs": [],
      "source": [
        "def carregando_dataframes():\n",
        "    daily_token = pd.read_parquet(daily_token_filepath)\n",
        "    employees = pd.read_csv(employees_filepath)\n",
        "    gyms = pd.read_csv(gyms_filepath)\n",
        "    freezes = pd.read_csv(freezes_filepath)\n",
        "    subscriptions = pd.read_csv(subscriptions_filepath)\n",
        "    subscriptions_features_dataset = pd.read_csv(subscriptions_features_dataset_filepath)\n",
        "    df_tendencia_de_utilizacao = pd.read_csv(tendencia_de_utilizacao_filepath)\n",
        "    df_cluster = pd.read_csv(cluster_filepath)\n",
        "    features = criar_features(daily_token, gyms, subscriptions)\n",
        "    return daily_token, employees, gyms, freezes, subscriptions, subscriptions_features_dataset, features, df_tendencia_de_utilizacao, df_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eZrqB04ui-g7"
      },
      "outputs": [],
      "source": [
        "def criar_df_atemporal(subscriptions_features_dataset, features):\n",
        "    df_final = pd.merge(subscriptions_features_dataset, features, on='employee_id', how='left')\n",
        "\n",
        "    # Selecionar colunas para clusterização\n",
        "    col_cluster = ['employee_id','type_mapped', 'gender_maped',\n",
        "                'Binario_fee', 'Fee', 'NumGymsWithinRadius',\n",
        "                'NumGymsNearCompany', 'distancia_cliente_empresa',\n",
        "                'payment_source_y']\n",
        "    df_atemp = df_final[col_cluster]\n",
        "    df_atemp['employee_id'] = df_atemp['employee_id'].astype(int)\n",
        "\n",
        "    return df_atemp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zIar7WhOi-g8"
      },
      "outputs": [],
      "source": [
        "def criar_df_temporal(CUT_OFF_DATE,dt,df_gyms,df_subscriptions,df_token,df_employees,df_freezes):\n",
        "    def calcular_gym_status(CUT_OFF_DATE,df,gyms):\n",
        "        today = CUT_OFF_DATE\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        df2 = df1[['validated_at','employee_id','token','gym_id']]\n",
        "        academias_mais_utilizadas = df2.groupby(['employee_id', 'gym_id']).size().reset_index(name='utilizacoes')\n",
        "        idx = academias_mais_utilizadas.groupby(['employee_id'])['utilizacoes'].transform(max) == academias_mais_utilizadas['utilizacoes']\n",
        "        academias_mais_utilizadas = academias_mais_utilizadas[idx]\n",
        "        gyms = gyms.drop('Unnamed: 0', axis=1)\n",
        "        gyms['registered_at'] = pd.to_datetime(gyms['registered_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['activated_at'] = pd.to_datetime(gyms['activated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['last_inactivated_at'] = pd.to_datetime(gyms['last_inactivated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms1 = gyms[gyms['registered_at'] <= today]\n",
        "        gyms2 = gyms1[['gym_id','gym_status']]\n",
        "        dfprinc = pd.merge(gyms2, academias_mais_utilizadas, on='gym_id', how='right')\n",
        "        return dfprinc\n",
        "    def calcular_age(CUT_OFF_DATE,df2):\n",
        "        df2['birthdate'] = pd.to_datetime(df2['birthdate'], errors='coerce')\n",
        "        df2 = df2.dropna(subset=['birthdate'])\n",
        "\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "\n",
        "        # iterando o df2 calculando a coluna 'age'\n",
        "        df2['age'] = df2.apply(lambda row: today.year - row['birthdate'].year - ((today.month < row['birthdate'].month) or ((today.month == row['birthdate'].month) and (today.day < row['birthdate'].day))), axis=1)\n",
        "\n",
        "        df2 = df2[['employee_id','age']]\n",
        "        return df2\n",
        "    def calcular_freezes(CUT_OFF_DATE,df1,df2):\n",
        "        df1['created_at'] = pd.to_datetime(df1['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        df1 = df1[df1['started_at'] < CUT_OFF_DATE]\n",
        "\n",
        "        distinct_ids_counts = df1.groupby('employee_id')['id'].nunique().reset_index()\n",
        "        distinct_ids_counts.columns = ['employee_id', 'freezes']\n",
        "\n",
        "        df2 = pd.merge(df2, distinct_ids_counts, on='employee_id', how='left')\n",
        "        df2['freezes'].fillna(0, inplace=True)\n",
        "\n",
        "        df2 = df2[['employee_id', 'freezes']]\n",
        "    def calcular_upgrade(CUT_OFF_DATE,df2):\n",
        "        df2['created_at'] = pd.to_datetime(df2['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df2['started_at'] = pd.to_datetime(df2['started_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        df2 = df2[df2['started_at'] < CUT_OFF_DATE]\n",
        "\n",
        "        df2['upgrade'] = np.where(df2['migrated_from_subscription_id'].notna(), 1, 0)\n",
        "\n",
        "        filtered_subscriptions = df2[df2['upgrade'] == 0]\n",
        "\n",
        "        count_subscriptions_by_employee = filtered_subscriptions.groupby('employee_id')['subscription_id'].count().reset_index()\n",
        "\n",
        "        count_subscriptions_by_employee = count_subscriptions_by_employee.rename(columns={'subscription_id': 'N° Subscriptions'})\n",
        "\n",
        "        df2 = pd.merge(df2, count_subscriptions_by_employee, on='employee_id', how='left')\n",
        "\n",
        "        df2 = df2[['employee_id', 'upgrade','N° Subscriptions']]\n",
        "\n",
        "        return df2\n",
        "    def calcular_pref_modality_utilizaçoes(CUT_OFF_DATE,df,gyms):\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        gyms = gyms.drop('Unnamed: 0', axis=1)\n",
        "        gyms['registered_at'] = pd.to_datetime(gyms['registered_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['activated_at'] = pd.to_datetime(gyms['activated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['last_inactivated_at'] = pd.to_datetime(gyms['last_inactivated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms1 = gyms[gyms['registered_at'] <= today]\n",
        "        df2 = df1[['employee_id','gym_id']]\n",
        "        gyms2 = gyms1[['gym_id','main_modality']]\n",
        "        gymsdf = pd.merge(gyms2, df2, how= 'right', on= ['gym_id'])\n",
        "        modalidades_preferidas = gymsdf.groupby(['employee_id', 'main_modality']).size().reset_index(name='num_main_modality')\n",
        "        idx1 = modalidades_preferidas.groupby(['employee_id'])['num_main_modality'].transform(max) == modalidades_preferidas['num_main_modality']\n",
        "        modalidades_preferidas = modalidades_preferidas[idx1]\n",
        "        modalidades_preferidas = modalidades_preferidas.rename(columns={'main_modality': 'pref_modality'})\n",
        "        gymsprinc = pd.merge(gymsdf, modalidades_preferidas[['employee_id', 'pref_modality', 'num_main_modality']],\n",
        "                on=['employee_id'], how='left')\n",
        "        return gymsprinc\n",
        "    def calcular_num_distinct_gyms(CUT_OFF_DATE,df):\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        df1['validated_at'] = pd.to_datetime(df1['validated_at'])\n",
        "        df1['created_at'] = pd.to_datetime(df1['created_at'])\n",
        "        df1 = df1.drop('Unnamed: 0', axis=1)\n",
        "        df2 = df1[['employee_id','gym_id']]\n",
        "        academias_distintas_por_usuario = df2.groupby('employee_id')['gym_id'].nunique().reset_index(name='num_distinct_gyms')\n",
        "        distpref = pd.merge(df2, academias_distintas_por_usuario, on='employee_id', how='left')\n",
        "        return distpref\n",
        "    def calcular_uses_per_week(CUT_OFF_DATE,df):\n",
        "        df['validated_at_minute'] = pd.to_datetime(df['validated_at_minute'],format = \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df1 = df[['employee_id','validated_at_minute']]\n",
        "        df1['Ano'] = df1['validated_at_minute'].dt.year\n",
        "        df1['Semana'] = df1['validated_at_minute'].dt.isocalendar().week\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df2 = df1[df1['validated_at_minute'] <= today]\n",
        "        num_utilizacoes_por_semana = df1.groupby(['employee_id', 'Ano', 'Semana']).size().reset_index(name='Num_Utilizacoes')\n",
        "        media_utilizacoes_por_semana = num_utilizacoes_por_semana.groupby('employee_id')['Num_Utilizacoes'].mean().reset_index(name='uses_per_week')\n",
        "        df3 = pd.merge(df2, media_utilizacoes_por_semana, how= 'left', on= ['employee_id'])\n",
        "        return df3\n",
        "    def calcular_most_frequent_hour(CUT_OFF_DATE,df):\n",
        "        df['validated_at_minute'] = pd.to_datetime(df['validated_at_minute'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        df1 = df[['employee_id','validated_at_minute']]\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df2 = df1[df1['validated_at_minute'] <= today]\n",
        "        hora_mais_frequente_por_usuario = df2.groupby('employee_id')['validated_at_minute'].agg(lambda x: x.dt.hour.mode().iloc[0])\n",
        "        df3 = pd.merge(df2, hora_mais_frequente_por_usuario, on='employee_id', how='left')\n",
        "        return df3\n",
        "    def active_at_date(CUT_OFF_DATE,df2):\n",
        "        df2['created_at'] = pd.to_datetime(df2['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df2['canceled_at'] = pd.to_datetime(df2['canceled_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        CUT_OFF_DATE = pd.to_datetime(CUT_OFF_DATE)\n",
        "\n",
        "        # Selecionando 1 apenas as assinaturas ativas na data de corte\n",
        "        df2['active_at_date'] = ((df2['created_at'] <= CUT_OFF_DATE) & ((df2['canceled_at'] >= CUT_OFF_DATE) | (df2['canceled_at'].isnull()))).astype(int)\n",
        "        # Para ser ativo,\n",
        "        # 1. A assinatura deve ter sido criada antes da data de corte\n",
        "        # 2. A assinatura deve ter sido cancelada depois da data de corte ou não ter sido cancelada\n",
        "\n",
        "        df2 = df2[['employee_id','active_at_date']]\n",
        "        return df2\n",
        "    def churn_at_dt(CUT_OFF_DATE,dt,df2):\n",
        "        CUT_OFF_DATE = pd.to_datetime(CUT_OFF_DATE)\n",
        "\n",
        "        df2['created_at'] = pd.to_datetime(df2['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df2['canceled_at'] = pd.to_datetime(df2['canceled_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        print(CUT_OFF_DATE+pd.Timedelta(dt))\n",
        "\n",
        "        # Selecionando 1 apenas as assinaturas ativas na data de corte\n",
        "        df2['churn_at_dt'] = ((df2['created_at'] <= CUT_OFF_DATE) & ((df2['canceled_at'] >= CUT_OFF_DATE) & (df2['canceled_at'] <= CUT_OFF_DATE + pd.Timedelta(days = dt)))).astype(int)\n",
        "        # Para dar churn no gap,\n",
        "        # 1. A assinatura deve ter sido criada antes da data de corte\n",
        "\n",
        "        df2 = df2[['employee_id','churn_at_dt']]\n",
        "        return df2\n",
        "    def calcular_normalized_stats(CUT_OFF_DATE,df):\n",
        "        today = CUT_OFF_DATE\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df['year'] = df['validated_at'].dt.year\n",
        "        df['month'] = df['validated_at'].dt.month\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        utilizacoes_por_mes = df1.groupby(['employee_id', 'year', 'month']).size().reset_index(name='num_utilizacoes')\n",
        "        tabela_pivo = utilizacoes_por_mes.pivot(index='employee_id', columns=['year', 'month'], values='num_utilizacoes').fillna(0)\n",
        "        tabela_pivo['variacao'] = tabela_pivo.diff(axis=1).fillna(0).sum(axis=1)\n",
        "        resultados_df = pd.DataFrame(columns=['user_id', 'estatistica_teste', 'valor_p', 'h0_rejeitada'])\n",
        "        def process_row(employee_id, row):\n",
        "            estatistica_teste, valor_p, h0_rejeitada, tendencia, x, y, z, w, t = original_test(row[:-1])\n",
        "            return {'user_id': employee_id, 'estatistica_teste': estatistica_teste, 'valor_p': valor_p, 'h0_rejeitada': h0_rejeitada, 'tendencia': tendencia}\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = []\n",
        "            for employee_id, row in tabela_pivo.iterrows():\n",
        "                futures.append(executor.submit(process_row, employee_id, row))\n",
        "            results = [future.result() for future in tqdm(futures, total=len(futures), desc=\"Processing\")]\n",
        "            resultados_df = pd.DataFrame(results)\n",
        "        resultados_df1 = resultados_df.drop('valor_p', axis=1)\n",
        "        resultados_df1.columns = ['employee_id', 'trend', 'p_value', 'normalized_stats']\n",
        "        return resultados_df1\n",
        "\n",
        "    # Carregando todas as funções com CUT_OFF_DATE\n",
        "    df_age = calcular_age(CUT_OFF_DATE,df_employees)\n",
        "    print('Calculou age')\n",
        "    df_upgrade = calcular_upgrade(CUT_OFF_DATE,df_subscriptions)\n",
        "    print('Calculou upgrade')\n",
        "    df_gym_status = calcular_gym_status(CUT_OFF_DATE,df_token,df_gyms)\n",
        "    print(\"Calculou gym status\")\n",
        "    df_pref_modality = calcular_pref_modality_utilizaçoes(CUT_OFF_DATE,df_token,df_gyms)\n",
        "    print(\"Calculou pref_modality\")\n",
        "    df_num_distinct_gyms = calcular_num_distinct_gyms(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou num_distinct_gyms\")\n",
        "    df_uses_per_week = calcular_uses_per_week(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou uses_per_week\")\n",
        "    df_most_frequent_hour = calcular_most_frequent_hour(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou most_frequent_hour\")\n",
        "    df_normalized_stats = calcular_normalized_stats(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou normalized_stats\")\n",
        "    df_active_at_date = active_at_date(CUT_OFF_DATE,df_subscriptions)\n",
        "    print('Calculou active_at_date')\n",
        "    df_churn_at_dt = churn_at_dt(CUT_OFF_DATE, dt,df_subscriptions)\n",
        "    print('Calculou churn_at_dt')\n",
        "\n",
        "    # Merge nos 3 DataFrames com chave 'employee_id'\n",
        "    df_temp = pd.merge(df_age, df_upgrade, on='employee_id', how='inner')\n",
        "    #print(\"Após o primeiro merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_active_at_date, on='employee_id', how='inner')\n",
        "    #print(\"Após o terceiro merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_churn_at_dt, on='employee_id', how='right')\n",
        "    #print(\"Após o quarto merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_gym_status, on='employee_id', how='inner')\n",
        "    #print(\"Após o quinto merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_pref_modality, on='employee_id', how='inner')\n",
        "    #print(\"Após o sexto merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_num_distinct_gyms, on='employee_id', how='inner')\n",
        "    #print(\"Após o sétimo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_uses_per_week, on='employee_id', how='inner')\n",
        "    #print(\"Após o oitavo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_most_frequent_hour, on='employee_id', how='inner')\n",
        "    #print(\"Após o nono merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_normalized_stats, on='employee_id', how='inner')\n",
        "    #print(\"Após o décimo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    return df_temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BPHL4LXni-g8"
      },
      "outputs": [],
      "source": [
        "def criar_df_inferencia(CUT_OFF_DATE,dt,df_gyms,df_subscriptions,df_token,df_employees,df_freezes):\n",
        "    def calcular_gym_status(CUT_OFF_DATE,df,gyms):\n",
        "        today = CUT_OFF_DATE\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        df2 = df1[['validated_at','employee_id','token','gym_id']]\n",
        "        academias_mais_utilizadas = df2.groupby(['employee_id', 'gym_id']).size().reset_index(name='utilizacoes')\n",
        "        idx = academias_mais_utilizadas.groupby(['employee_id'])['utilizacoes'].transform(max) == academias_mais_utilizadas['utilizacoes']\n",
        "        academias_mais_utilizadas = academias_mais_utilizadas[idx]\n",
        "        gyms = gyms.drop('Unnamed: 0', axis=1)\n",
        "        gyms['registered_at'] = pd.to_datetime(gyms['registered_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['activated_at'] = pd.to_datetime(gyms['activated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['last_inactivated_at'] = pd.to_datetime(gyms['last_inactivated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms1 = gyms[gyms['registered_at'] <= today]\n",
        "        gyms2 = gyms1[['gym_id','gym_status']]\n",
        "        dfprinc = pd.merge(gyms2, academias_mais_utilizadas, on='gym_id', how='right')\n",
        "        return dfprinc\n",
        "    def calcular_age(CUT_OFF_DATE,df2):\n",
        "        df2['birthdate'] = pd.to_datetime(df2['birthdate'], errors='coerce')\n",
        "        df2 = df2.dropna(subset=['birthdate'])\n",
        "\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "\n",
        "        # iterando o df2 calculando a coluna 'age'\n",
        "        df2['age'] = df2.apply(lambda row: today.year - row['birthdate'].year - ((today.month < row['birthdate'].month) or ((today.month == row['birthdate'].month) and (today.day < row['birthdate'].day))), axis=1)\n",
        "\n",
        "        df2 = df2[['employee_id','age']]\n",
        "        return df2\n",
        "    def calcular_freezes(CUT_OFF_DATE,df1,df2):\n",
        "        df1['created_at'] = pd.to_datetime(df1['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        df1 = df1[df1['started_at'] < CUT_OFF_DATE]\n",
        "\n",
        "        distinct_ids_counts = df1.groupby('employee_id')['id'].nunique().reset_index()\n",
        "        distinct_ids_counts.columns = ['employee_id', 'freezes']\n",
        "\n",
        "        df2 = pd.merge(df2, distinct_ids_counts, on='employee_id', how='left')\n",
        "        df2['freezes'].fillna(0, inplace=True)\n",
        "\n",
        "        df2 = df2[['employee_id', 'freezes']]\n",
        "    def calcular_upgrade(CUT_OFF_DATE,df2):\n",
        "        df2['created_at'] = pd.to_datetime(df2['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df2['started_at'] = pd.to_datetime(df2['started_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        df2 = df2[df2['started_at'] < CUT_OFF_DATE]\n",
        "\n",
        "        df2['upgrade'] = np.where(df2['migrated_from_subscription_id'].notna(), 1, 0)\n",
        "\n",
        "        filtered_subscriptions = df2[df2['upgrade'] == 0]\n",
        "\n",
        "        count_subscriptions_by_employee = filtered_subscriptions.groupby('employee_id')['subscription_id'].count().reset_index()\n",
        "\n",
        "        count_subscriptions_by_employee = count_subscriptions_by_employee.rename(columns={'subscription_id': 'N° Subscriptions'})\n",
        "\n",
        "        df2 = pd.merge(df2, count_subscriptions_by_employee, on='employee_id', how='left')\n",
        "\n",
        "        df2 = df2[['employee_id', 'upgrade','N° Subscriptions']]\n",
        "\n",
        "        return df2\n",
        "    def calcular_pref_modality_utilizaçoes(CUT_OFF_DATE,df,gyms):\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        gyms = gyms.drop('Unnamed: 0', axis=1)\n",
        "        gyms['registered_at'] = pd.to_datetime(gyms['registered_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['activated_at'] = pd.to_datetime(gyms['activated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms['last_inactivated_at'] = pd.to_datetime(gyms['last_inactivated_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        gyms1 = gyms[gyms['registered_at'] <= today]\n",
        "        df2 = df1[['employee_id','gym_id']]\n",
        "        gyms2 = gyms1[['gym_id','main_modality']]\n",
        "        gymsdf = pd.merge(gyms2, df2, how= 'right', on= ['gym_id'])\n",
        "        modalidades_preferidas = gymsdf.groupby(['employee_id', 'main_modality']).size().reset_index(name='num_main_modality')\n",
        "        idx1 = modalidades_preferidas.groupby(['employee_id'])['num_main_modality'].transform(max) == modalidades_preferidas['num_main_modality']\n",
        "        modalidades_preferidas = modalidades_preferidas[idx1]\n",
        "        modalidades_preferidas = modalidades_preferidas.rename(columns={'main_modality': 'pref_modality'})\n",
        "        gymsprinc = pd.merge(gymsdf, modalidades_preferidas[['employee_id', 'pref_modality', 'num_main_modality']],\n",
        "                on=['employee_id'], how='left')\n",
        "        return gymsprinc\n",
        "    def calcular_num_distinct_gyms(CUT_OFF_DATE,df):\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        df1['validated_at'] = pd.to_datetime(df1['validated_at'])\n",
        "        df1['created_at'] = pd.to_datetime(df1['created_at'])\n",
        "        df1 = df1.drop('Unnamed: 0', axis=1)\n",
        "        df2 = df1[['employee_id','gym_id']]\n",
        "        academias_distintas_por_usuario = df2.groupby('employee_id')['gym_id'].nunique().reset_index(name='num_distinct_gyms')\n",
        "        distpref = pd.merge(df2, academias_distintas_por_usuario, on='employee_id', how='left')\n",
        "        return distpref\n",
        "    def calcular_uses_per_week(CUT_OFF_DATE,df):\n",
        "        df['validated_at_minute'] = pd.to_datetime(df['validated_at_minute'],format = \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df1 = df[['employee_id','validated_at_minute']]\n",
        "        df1['Ano'] = df1['validated_at_minute'].dt.year\n",
        "        df1['Semana'] = df1['validated_at_minute'].dt.isocalendar().week\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df2 = df1[df1['validated_at_minute'] <= today]\n",
        "        num_utilizacoes_por_semana = df1.groupby(['employee_id', 'Ano', 'Semana']).size().reset_index(name='Num_Utilizacoes')\n",
        "        media_utilizacoes_por_semana = num_utilizacoes_por_semana.groupby('employee_id')['Num_Utilizacoes'].mean().reset_index(name='uses_per_week')\n",
        "        df3 = pd.merge(df2, media_utilizacoes_por_semana, how= 'left', on= ['employee_id'])\n",
        "        return df3\n",
        "    def calcular_most_frequent_hour(CUT_OFF_DATE,df):\n",
        "        df['validated_at_minute'] = pd.to_datetime(df['validated_at_minute'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        df1 = df[['employee_id','validated_at_minute']]\n",
        "        today = pd.to_datetime(CUT_OFF_DATE)\n",
        "        df2 = df1[df1['validated_at_minute'] <= today]\n",
        "        hora_mais_frequente_por_usuario = df2.groupby('employee_id')['validated_at_minute'].agg(lambda x: x.dt.hour.mode().iloc[0])\n",
        "        df3 = pd.merge(df2, hora_mais_frequente_por_usuario, on='employee_id', how='left')\n",
        "        return df3\n",
        "    def active_at_date(CUT_OFF_DATE,df2):\n",
        "        df2['created_at'] = pd.to_datetime(df2['created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "        df2['canceled_at'] = pd.to_datetime(df2['canceled_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "        CUT_OFF_DATE = pd.to_datetime(CUT_OFF_DATE)\n",
        "\n",
        "        # Selecionando 1 apenas as assinaturas ativas na data de corte\n",
        "        df2['active_at_date'] = ((df2['created_at'] <= CUT_OFF_DATE) & ((df2['canceled_at'] >= CUT_OFF_DATE) | (df2['canceled_at'].isnull()))).astype(int)\n",
        "        # Para ser ativo,\n",
        "        # 1. A assinatura deve ter sido criada antes da data de corte\n",
        "        # 2. A assinatura deve ter sido cancelada depois da data de corte ou não ter sido cancelada\n",
        "\n",
        "        df2 = df2[['employee_id','active_at_date']]\n",
        "        return df2\n",
        "    def calcular_normalized_stats(CUT_OFF_DATE,df):\n",
        "        today = CUT_OFF_DATE\n",
        "        df['validated_at'] = pd.to_datetime(df['validated_at'])\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "        df = df.drop('Unnamed: 0', axis=1)\n",
        "        df['year'] = df['validated_at'].dt.year\n",
        "        df['month'] = df['validated_at'].dt.month\n",
        "        df1 = df[df['validated_at'] <= today]\n",
        "        utilizacoes_por_mes = df1.groupby(['employee_id', 'year', 'month']).size().reset_index(name='num_utilizacoes')\n",
        "        tabela_pivo = utilizacoes_por_mes.pivot(index='employee_id', columns=['year', 'month'], values='num_utilizacoes').fillna(0)\n",
        "        tabela_pivo['variacao'] = tabela_pivo.diff(axis=1).fillna(0).sum(axis=1)\n",
        "        resultados_df = pd.DataFrame(columns=['user_id', 'estatistica_teste', 'valor_p', 'h0_rejeitada'])\n",
        "        def process_row(employee_id, row):\n",
        "            estatistica_teste, valor_p, h0_rejeitada, tendencia, x, y, z, w, t = original_test(row[:-1])\n",
        "            return {'user_id': employee_id, 'estatistica_teste': estatistica_teste, 'valor_p': valor_p, 'h0_rejeitada': h0_rejeitada, 'tendencia': tendencia}\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = []\n",
        "            for employee_id, row in tabela_pivo.iterrows():\n",
        "                futures.append(executor.submit(process_row, employee_id, row))\n",
        "            results = [future.result() for future in tqdm(futures, total=len(futures), desc=\"Processing\")]\n",
        "            resultados_df = pd.DataFrame(results)\n",
        "        resultados_df1 = resultados_df.drop('valor_p', axis=1)\n",
        "        resultados_df1.columns = ['employee_id', 'trend', 'p_value', 'normalized_stats']\n",
        "        return resultados_df1\n",
        "\n",
        "    # Carregando todas as funções com CUT_OFF_DATE\n",
        "    df_age = calcular_age(CUT_OFF_DATE,df_employees)\n",
        "    print('Calculou age')\n",
        "    df_upgrade = calcular_upgrade(CUT_OFF_DATE,df_subscriptions)\n",
        "    print('Calculou upgrade')\n",
        "    df_gym_status = calcular_gym_status(CUT_OFF_DATE,df_token,df_gyms)\n",
        "    print(\"Calculou gym status\")\n",
        "    df_pref_modality = calcular_pref_modality_utilizaçoes(CUT_OFF_DATE,df_token,df_gyms)\n",
        "    print(\"Calculou pref_modality\")\n",
        "    df_num_distinct_gyms = calcular_num_distinct_gyms(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou num_distinct_gyms\")\n",
        "    df_uses_per_week = calcular_uses_per_week(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou uses_per_week\")\n",
        "    df_most_frequent_hour = calcular_most_frequent_hour(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou most_frequent_hour\")\n",
        "    df_normalized_stats = calcular_normalized_stats(CUT_OFF_DATE,df_token)\n",
        "    print(\"Calculou normalized_stats\")\n",
        "    df_active_at_date = active_at_date(CUT_OFF_DATE,df_subscriptions)\n",
        "    print('Calculou active_at_date')\n",
        "\n",
        "    # Merge nos 3 DataFrames com chave 'employee_id'\n",
        "    df_temp = pd.merge(df_age, df_upgrade, on='employee_id', how='inner')\n",
        "    #print(\"Após o primeiro merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_active_at_date, on='employee_id', how='inner')\n",
        "    #print(\"Após o terceiro merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_gym_status, on='employee_id', how='inner')\n",
        "    #print(\"Após o quinto merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_pref_modality, on='employee_id', how='inner')\n",
        "    #print(\"Após o sexto merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_num_distinct_gyms, on='employee_id', how='inner')\n",
        "    #print(\"Após o sétimo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_uses_per_week, on='employee_id', how='inner')\n",
        "    #print(\"Após o oitavo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_most_frequent_hour, on='employee_id', how='inner')\n",
        "    #print(\"Após o nono merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    df_temp = pd.merge(df_temp, df_normalized_stats, on='employee_id', how='inner')\n",
        "    #print(\"Após o décimo merge:\", df_temp.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df_temp = df_temp.drop_duplicates(subset='employee_id', keep='first')\n",
        "    #print(\"Após remover duplicatas:\", df_temp.shape)\n",
        "\n",
        "    return df_temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3WzhInlKi-g-"
      },
      "outputs": [],
      "source": [
        "def merge_dfs(df_atemp, df_temp):\n",
        "    df = pd.merge(df_atemp, df_temp, on='employee_id', how='left')\n",
        "    print(\"Após o merge:\", df.shape)\n",
        "\n",
        "    # Remove duplicatas:\n",
        "    df = df.drop_duplicates(subset='employee_id', keep='first')\n",
        "    print(\"Após remover duplicatas:\", df.shape)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2h2qvA0hi-hA"
      },
      "outputs": [],
      "source": [
        "def preprocessa_df(df):\n",
        "    # dropar nulos com base na coluna 'active_at_date'\n",
        "    df = df.dropna(subset=['active_at_date'])\n",
        "\n",
        "    # Substitua active por 1 e inactive por 0 em gym status:\n",
        "    df['gym_status'] = df['gym_status'].map({'active': 1, 'inactive': 0})\n",
        "    # Substitua musculação por 1 e qualquer outra categoria por 0 em pref modality:\n",
        "    df['pref_modality'] = df['pref_modality'].map(lambda x: 1 if x == 'MUSCULAÇÃO' else 0)\n",
        "    # Substitua payroll por 0, prepaid por 1 e own_wallet por 2 em payment source:\n",
        "    df['payment_source_y'] = df['payment_source_y'].map({'payroll_deduction': 0, 'prepaid': 1, 'own_wallet': 2,})\n",
        "\n",
        "    # Preenchendo valores nulos com 0\n",
        "    df['Binario_fee'].fillna(0, inplace=True)\n",
        "    df['Fee'].fillna(0, inplace=True)\n",
        "    df['upgrade'].fillna(0, inplace=True)\n",
        "    df['N° Subscriptions'].fillna(0, inplace=True)\n",
        "    df['payment_source_y'].fillna(0, inplace=True)\n",
        "    df['pref_modality'].fillna(1, inplace=True) # Preenche com musculação\n",
        "\n",
        "    # Preenchendo valores nulos com a média\n",
        "    media_distancia = df['distancia_cliente_empresa'].mean()\n",
        "    media_idade = df['age'].mean()\n",
        "    media_genero = df['gender_maped'].mean()\n",
        "\n",
        "    # Preenche os valores NaN com as médias calculadas\n",
        "    df['distancia_cliente_empresa'].fillna(media_distancia, inplace=True)\n",
        "    df['age'].fillna(media_idade, inplace=True)\n",
        "    df['gender_maped'].fillna(media_genero, inplace=True)\n",
        "    df.dropna(subset=['type_mapped', 'NumGymsWithinRadius'], inplace=True)\n",
        "\n",
        "\n",
        "    '''\n",
        "    df = df[['employee_id','churn_at_dt','active_at_date','NumGymsWithinRadius', 'NumGymsNearCompany',\n",
        "    'distancia_cliente_empresa', 'payment_source_y', 'age', 'upgrade',\n",
        "    'N° Subscriptions', 'freezes', 'gym_status', 'utilizacoes',\n",
        "    'pref_modality', 'num_main_modality',\n",
        "    'num_distinct_gyms',\n",
        "    'uses_per_week',\n",
        "    'normalized_stats','type_mapped', 'gender_maped', 'Binario_fee', 'Fee']]\n",
        "    '''\n",
        "\n",
        "    df = df[['employee_id','churn_at_dt','active_at_date','NumGymsWithinRadius', 'NumGymsNearCompany',\n",
        "    'distancia_cliente_empresa', 'payment_source_y', 'age', 'upgrade',\n",
        "    'N° Subscriptions','type_mapped', 'gender_maped', 'Binario_fee', 'Fee']]\n",
        "\n",
        "    df = df[df['active_at_date'] == 1]\n",
        "\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "reFKjOMJi-hA"
      },
      "outputs": [],
      "source": [
        "def preprocessa_df_inferencia(df):\n",
        "    # dropar nulos com base na coluna 'active_at_date'\n",
        "    df = df.dropna(subset=['active_at_date'])\n",
        "\n",
        "    # Substitua active por 1 e inactive por 0 em gym status:\n",
        "    df['gym_status'] = df['gym_status'].map({'active': 1, 'inactive': 0})\n",
        "    # Substitua musculação por 1 e qualquer outra categoria por 0 em pref modality:\n",
        "    df['pref_modality'] = df['pref_modality'].map(lambda x: 1 if x == 'MUSCULAÇÃO' else 0)\n",
        "    # Substitua payroll por 0, prepaid por 1 e own_wallet por 2 em payment source:\n",
        "    df['payment_source_y'] = df['payment_source_y'].map({'payroll_deduction': 0, 'prepaid': 1, 'own_wallet': 2,})\n",
        "\n",
        "    # Preenchendo valores nulos com 0\n",
        "    df['Binario_fee'].fillna(0, inplace=True)\n",
        "    df['Fee'].fillna(0, inplace=True)\n",
        "    df['upgrade'].fillna(0, inplace=True)\n",
        "    df['N° Subscriptions'].fillna(0, inplace=True)\n",
        "    df['payment_source_y'].fillna(0, inplace=True)\n",
        "    df['pref_modality'].fillna(1, inplace=True) # Preenche com musculação\n",
        "\n",
        "    # Preenchendo valores nulos com a média\n",
        "    media_distancia = df['distancia_cliente_empresa'].mean()\n",
        "    media_idade = df['age'].mean()\n",
        "    media_genero = df['gender_maped'].mean()\n",
        "\n",
        "    # Preenche os valores NaN com as médias calculadas\n",
        "    df['distancia_cliente_empresa'].fillna(media_distancia, inplace=True)\n",
        "    df['age'].fillna(media_idade, inplace=True)\n",
        "    df['gender_maped'].fillna(media_genero, inplace=True)\n",
        "    df.dropna(subset=['type_mapped', 'NumGymsWithinRadius'], inplace=True)\n",
        "\n",
        "\n",
        "    '''\n",
        "    df = df[['employee_id','churn_at_dt','active_at_date','NumGymsWithinRadius', 'NumGymsNearCompany',\n",
        "    'distancia_cliente_empresa', 'payment_source_y', 'age', 'upgrade',\n",
        "    'N° Subscriptions', 'freezes', 'gym_status', 'utilizacoes',\n",
        "    'pref_modality', 'num_main_modality',\n",
        "    'num_distinct_gyms',\n",
        "    'uses_per_week',\n",
        "    'normalized_stats','type_mapped', 'gender_maped', 'Binario_fee', 'Fee']]\n",
        "    '''\n",
        "\n",
        "    df = df[['employee_id','active_at_date','NumGymsWithinRadius', 'NumGymsNearCompany',\n",
        "    'distancia_cliente_empresa', 'payment_source_y', 'age', 'upgrade',\n",
        "    'N° Subscriptions','type_mapped', 'gender_maped', 'Binario_fee', 'Fee']]\n",
        "\n",
        "    df = df[df['active_at_date'] == 1]\n",
        "    df = df.drop('active_at_date', axis=1)\n",
        "\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qo6q5TwGi-hB"
      },
      "outputs": [],
      "source": [
        "def treinar_modelo(df, clusterizado ,df_cluster, dt):\n",
        "    # se o modelo for clusterizado, vamos criar 8 modelos diferentes, 1 para cada cluster:\n",
        "    if clusterizado:\n",
        "        modelos = []\n",
        "        for i in range(8):\n",
        "            # filtrar df apenas para os employees que estão no cluster i\n",
        "            df_cluster_especifico = df_cluster[df_cluster['cluster'] == i]\n",
        "            df_inferencia = df[df['employee_id'].isin(df_cluster_especifico['employee_id'])]\n",
        "            # Definindo as variáveis preditoras e a variável resposta\n",
        "            X = df_inferencia.drop(['employee_id', 'churn_at_dt','active_at_date'], axis=1)\n",
        "            # Normalizando os dados\n",
        "            scaler_clusterizado = MinMaxScaler()\n",
        "            X = scaler_clusterizado.fit_transform(X)\n",
        "            y = df_inferencia['churn_at_dt']\n",
        "\n",
        "            # Dividindo os dados em treino e teste\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "            # Métodos de tratamento de desbalanceamento\n",
        "            methods = ['None','Oversampling', 'Undersampling', 'SMOTE']\n",
        "\n",
        "            # Definindo o método de desbalanceamento\n",
        "            if dt < 100:\n",
        "              method = 'Undersampling'\n",
        "            else:\n",
        "              method = 'None'\n",
        "\n",
        "            if method == 'Oversampling':\n",
        "                # Aplicando oversampling\n",
        "                sampler = RandomOverSampler(random_state=42)\n",
        "            elif method == 'Undersampling':\n",
        "                # Aplicando undersampling\n",
        "                sampler = RandomUnderSampler(random_state=42)\n",
        "            elif method == 'SMOTE':\n",
        "                # Aplicando SMOTE\n",
        "                sampler = SMOTE(random_state=42)\n",
        "\n",
        "                # Aplicando o método de amostragem\n",
        "                if method != 'None':\n",
        "                    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "                else:\n",
        "                    X_train, y_train = X_train, y_train\n",
        "\n",
        "            # Instanciando o modelo de nome\n",
        "            logreg = RandomForestClassifier()\n",
        "\n",
        "            # Treinando o modelo\n",
        "            logreg.fit(X_train, y_train)\n",
        "\n",
        "            # Fazendo previsões\n",
        "            y_pred = logreg.predict(X_test)\n",
        "            Y_probabilities = logreg.predict_proba(X_test)[:, 1]  # Probabilidade prevista para a classe positiva\n",
        "\n",
        "            results = pd.DataFrame({'churn_at_dt': y_test,\n",
        "                                    'churn probability': Y_probabilities,'prediction': y_pred,'employee_cluster': i})\n",
        "            # export results with dt and cluster\n",
        "            results.to_csv(f'results_{dt}_cluster_{i}.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "            # Avaliando o modelo\n",
        "            print('Acurácia:', accuracy_score(y_test, y_pred))\n",
        "            print(confusion_matrix(y_test, y_pred))\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            modelos.append(logreg)\n",
        "        return modelos, scaler_clusterizado\n",
        "    else:\n",
        "        # Definindo as variáveis preditoras e a variável resposta\n",
        "        X = df.drop(['employee_id', 'churn_at_dt','active_at_date'], axis=1)\n",
        "        # Normalizando os dados\n",
        "        scaler_geral = MinMaxScaler()\n",
        "        X = scaler_geral.fit_transform(X)\n",
        "        y = df['churn_at_dt']\n",
        "\n",
        "        # Dividindo os dados em treino e teste\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Métodos de tratamento de desbalanceamento\n",
        "        methods = ['None','Oversampling', 'Undersampling', 'SMOTE']\n",
        "\n",
        "        # Definindo o método de desbalanceamento\n",
        "        if dt < 100:\n",
        "          method = 'Undersampling'\n",
        "        else:\n",
        "          method = 'None'\n",
        "\n",
        "        if method == 'Oversampling':\n",
        "            # Aplicando oversampling\n",
        "            sampler = RandomOverSampler(random_state=42)\n",
        "        elif method == 'Undersampling':\n",
        "            # Aplicando undersampling\n",
        "            sampler = RandomUnderSampler(random_state=42)\n",
        "        elif method == 'SMOTE':\n",
        "            # Aplicando SMOTE\n",
        "            sampler = SMOTE(random_state=42)\n",
        "\n",
        "            # Aplicando o método de amostragem\n",
        "            if method != 'None':\n",
        "                X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "            else:\n",
        "                X_train, y_train = X_train, y_train\n",
        "\n",
        "        # Instanciando o modelo de nome\n",
        "        logreg = RandomForestClassifier()\n",
        "\n",
        "        # Treinando o modelo\n",
        "        logreg.fit(X_train, y_train)\n",
        "\n",
        "        # Fazendo previsões\n",
        "        y_pred = logreg.predict(X_test)\n",
        "        Y_probabilities = logreg.predict_proba(X_test)[:, 1]  # Probabilidade prevista para a classe positiva\n",
        "\n",
        "        results = pd.DataFrame({'churn_at_dt': y_test, 'churn probability': Y_probabilities,'prediction': y_pred})\n",
        "        # export results with dt and cluster\n",
        "        results.to_csv(f'results_{dt}_padrao.csv', index=False)\n",
        "\n",
        "        # Avaliando o modelo\n",
        "        print('Acurácia:', accuracy_score(y_test, y_pred))\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        return logreg, scaler_geral\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Eq6kr_oRi-hB"
      },
      "outputs": [],
      "source": [
        "def treinar_clusterizador(df_subscriptions_features_dataset,df_tendencia_de_utilizacao,df_features):\n",
        "    df = pd.merge(df_subscriptions_features_dataset, df_tendencia_de_utilizacao, on='employee_id', how='outer')\n",
        "    df = pd.merge(df, df_features, on='employee_id', how='outer')\n",
        "    df = df.drop_duplicates(subset='employee_id', keep='first')\n",
        "    col_cluster = ['employee_id','type_mapped', 'Age', 'gender_maped', 'upgrade', 'count_subscriptions',\n",
        "                'numero_de_ids_distintos', 'Binario_fee', 'Fee', 'NumGymsWithinRadius',\n",
        "                'NumGymsNearCompany', 'distancia_cliente_empresa','normalized_stats',\n",
        "                'gym_status','utilizacoes','pref_modality','num_distinct_gyms',\n",
        "                'payment_source_y','most_frequent_hour','uses_per_week']\n",
        "    df_filtrado = df[col_cluster]\n",
        "\n",
        "    # Substitua active por 1 e inactive por 0 em gym status:\n",
        "    df_filtrado['gym_status'] = df_filtrado['gym_status'].map({'active': 1, 'inactive': 0})\n",
        "    # Substitua musculação por 1 e qualquer outra categoria por 0 em pref modality:\n",
        "    df_filtrado['pref_modality'] = df_filtrado['pref_modality'].map(lambda x: 1 if x == 'MUSCULAÇÃO' else 0)\n",
        "    # Substitua payroll por 0, prepaid por 1 e own_wallet por 2 em payment source:\n",
        "    df_filtrado['payment_source_y'] = df_filtrado['payment_source_y'].map({'payroll_deduction': 0, 'prepaid': 1, 'own_wallet': 2})\n",
        "\n",
        "    # Preenchendo valores nulos com 0\n",
        "    df_filtrado['Binario_fee'].fillna(0, inplace=True)\n",
        "    df_filtrado['Fee'].fillna(0, inplace=True)\n",
        "    df_filtrado['pref_modality'].fillna(1, inplace=True) # Preenche com musculação\n",
        "\n",
        "    # Preenchendo valores nulos com a média\n",
        "    media_distancia = df_filtrado['distancia_cliente_empresa'].mean()\n",
        "    media_idade = df_filtrado['Age'].mean()\n",
        "    media_genero = df_filtrado['gender_maped'].mean()\n",
        "\n",
        "    # Preenche os valores NaN com as médias calculadas\n",
        "    df_filtrado['distancia_cliente_empresa'].fillna(media_distancia, inplace=True)\n",
        "    df_filtrado['Age'].fillna(media_idade, inplace=True)\n",
        "    df_filtrado['gender_maped'].fillna(media_genero, inplace=True)\n",
        "    df_filtrado.fillna(0, inplace=True)\n",
        "\n",
        "    # Normalizando df_filtrado sem a coluna 'employee_id'\n",
        "    scaler_clusterizador = MinMaxScaler()\n",
        "    df_scaled = scaler_clusterizador.fit_transform(df_filtrado.drop('employee_id', axis=1))\n",
        "\n",
        "    # Criando um dataframe com a coluna employee_id de df filtrado e as colunas normalizadas\n",
        "    df_clusterizacao = pd.concat([pd.DataFrame(df_filtrado['employee_id']), pd.DataFrame(df_scaled)], axis=1)\n",
        "\n",
        "    # Selecionando colunas específicas do df_clusterização\n",
        "    df_clusterizacao = df_clusterizacao.iloc[:,[1,2,3,4,9,10,11,12,14,15,17,18,19]] # 13 das 19 colunas\n",
        "\n",
        "    # Realizando uma amostragem de 160 000 dados\n",
        "    df_selected = pd.DataFrame(df_scaled).sample(n=16000, random_state=42)\n",
        "\n",
        "    # Selecionando colunas específicas do df_scaled:\n",
        "    df_selected = df_selected.iloc[:,[0,1,2,3,8,9,10,11,13,14,16,17,18]] # 13 das 19 colunas\n",
        "\n",
        "    kmeans_algorithm = KMeans(n_clusters=8, n_init=10, random_state=0)\n",
        "    kmeans_algorithm.fit(df_selected)\n",
        "\n",
        "    return kmeans_algorithm, df_clusterizacao\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nrS1WBhBi-hC"
      },
      "outputs": [],
      "source": [
        "def previsao(dt_interval):\n",
        "    df_token, df_employees, df_gyms, df_freezes, df_subscriptions, df_subscriptions_features_dataset, df_features, df_tendencia_de_utilizacao, df_cluster = carregando_dataframes()\n",
        "    print('Dataframes carregados')\n",
        "    df_atemp = criar_df_atemporal(df_subscriptions_features_dataset,df_features)\n",
        "    print('DF Atemporal criado')\n",
        "    dfs = []\n",
        "    for dt in dt_interval:\n",
        "        df_temp = criar_df_temporal(CUT_OFF_DATE,dt,df_gyms,df_subscriptions,df_token,df_employees,df_freezes)\n",
        "        print('DF Temporal criado')\n",
        "        #df_inferencia_temporal = criar_df_inferencia(DATE_OF_INFERENCE,dt,df_gyms,df_subscriptions,df_token,df_employees,df_freezes)\n",
        "        #df_inferencia = merge_dfs(df_atemp, df_inferencia_temporal)\n",
        "        #df_inferencia = preprocessa_df_inferencia(df_inferencia)\n",
        "        print('DF Inferencia criado')\n",
        "        #clusterizador, df_clusterizacao = treinar_clusterizador(df_subscriptions_features_dataset,df_tendencia_de_utilizacao,df_features)\n",
        "        print('Clusterizador criado')\n",
        "        df_treino = merge_dfs(df_atemp, df_temp)\n",
        "        print('DF Mergeado')\n",
        "        df_treino = preprocessa_df(df_treino)\n",
        "        print('DF Preprocessado')\n",
        "        modelo_geral, scaler_geral= treinar_modelo(df_treino,0,df_cluster, dt)\n",
        "        print('Modelo Geral Treinado')\n",
        "        modelos_clusters, scaler_clusterizado = treinar_modelo(df_treino, 1, df_cluster, dt)\n",
        "        print('Modelos Clusters Treinados')\n",
        "\n",
        "        \"\"\"\n",
        "        # MODELO GERAL:\n",
        "        employee_list = []\n",
        "        previsoes_geral = []\n",
        "        previsoes_cluster = []\n",
        "        cluster_employee_list = []\n",
        "        for index, row in df_inferencia.iterrows():\n",
        "            # verifica o cluster de employee_id e seleciona o modelo correspondente\n",
        "            try:\n",
        "              cluster_employee = clusterizador.predict(df_clusterizacao[df_clusterizacao.index==row['employee_id']].values.reshape(1, -1))[0]\n",
        "            except:\n",
        "              cluster_employee = 0\n",
        "            cluster_employee_list.append(cluster_employee)\n",
        "            employee_list.append(row['employee_id'])\n",
        "            X_inferencia = row.drop(['employee_id'])\n",
        "            X_inferencia = X_inferencia.values.reshape(1, -1)\n",
        "            # Aplicando scalers\n",
        "            X_inferencia_geral = scaler_geral.transform(X_inferencia)\n",
        "            X_inferencia_clusterizado = scaler_clusterizado.transform(X_inferencia)\n",
        "\n",
        "            previsao_geral = modelo_geral.predict_proba(X_inferencia_geral)\n",
        "            previsao_clusterizado = modelos_clusters[cluster_employee].predict_proba(X_inferencia_clusterizado)\n",
        "            previsoes_geral.append(previsao_geral[0][1] * 100)\n",
        "            previsoes_cluster.append(previsao_clusterizado[0][1] * 100)\n",
        "        df_previsoes = pd.DataFrame({'employee_id': employee_list, 'cluster_employee':cluster_employee_list, f'churn_at_{dt}_geral': previsoes_geral, f'churn_at_{dt}_cluster':previsoes_cluster})\n",
        "        dfs.append(df_previsoes)\n",
        "\n",
        "    # Inicialize o DataFrame final com o primeiro DataFrame na lista dfs\n",
        "    df_final = dfs[0]\n",
        "\n",
        "    # Loop sobre os DataFrames restantes na lista dfs\n",
        "    for df in dfs[1:]:\n",
        "        # Mesclar o DataFrame atual com o DataFrame final usando 'employee_id' como chave de junção\n",
        "        df_final = pd.merge(df_final, df, on='employee_id')\n",
        "\n",
        "        \"\"\"\n",
        "    df_final = pd.DataFrame()\n",
        "    return df_final"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFcl1mNYJbt"
      },
      "source": [
        "Use a célula abaixo para, só com ela, rodar todas as funções uma por uma.\n",
        "\n",
        "A função previsao é uma função orquestradora, que vai chamando cada função, uma por uma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MrvgEPWei-hC",
        "outputId": "6124443d-787d-4d53-bfab-08c7941b27e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframes carregados\n",
            "DF Atemporal criado\n",
            "Calculou age\n",
            "Calculou upgrade\n",
            "Calculou gym status\n",
            "Calculou pref_modality\n",
            "Calculou num_distinct_gyms\n",
            "Calculou uses_per_week\n",
            "Calculou most_frequent_hour\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 407769/407769 [07:32<00:00, 902.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculou normalized_stats\n",
            "Calculou active_at_date\n",
            "2022-11-17 00:00:00.000000090\n",
            "Calculou churn_at_dt\n",
            "DF Temporal criado\n",
            "DF Inferencia criado\n",
            "Clusterizador criado\n",
            "Após o merge: (1764656, 32)\n",
            "Após remover duplicatas: (1136775, 32)\n",
            "DF Mergeado\n",
            "DF Preprocessado\n",
            "Acurácia: 0.7672029851943988\n",
            "[[37199  1645]\n",
            " [ 9959  1043]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87     38844\n",
            "         1.0       0.39      0.09      0.15     11002\n",
            "\n",
            "    accuracy                           0.77     49846\n",
            "   macro avg       0.59      0.53      0.51     49846\n",
            "weighted avg       0.70      0.77      0.71     49846\n",
            "\n",
            "Modelo Geral Treinado\n",
            "Acurácia: 0.7981708262284917\n",
            "[[4933  186]\n",
            " [1116  216]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.96      0.88      5119\n",
            "         1.0       0.54      0.16      0.25      1332\n",
            "\n",
            "    accuracy                           0.80      6451\n",
            "   macro avg       0.68      0.56      0.57      6451\n",
            "weighted avg       0.76      0.80      0.75      6451\n",
            "\n",
            "Acurácia: 0.7755303404045387\n",
            "[[3081  112]\n",
            " [ 798   63]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87      3193\n",
            "         1.0       0.36      0.07      0.12       861\n",
            "\n",
            "    accuracy                           0.78      4054\n",
            "   macro avg       0.58      0.52      0.50      4054\n",
            "weighted avg       0.70      0.78      0.71      4054\n",
            "\n",
            "Acurácia: 0.7437548790007806\n",
            "[[7283  420]\n",
            " [2206  339]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.95      0.85      7703\n",
            "         1.0       0.45      0.13      0.21      2545\n",
            "\n",
            "    accuracy                           0.74     10248\n",
            "   macro avg       0.61      0.54      0.53     10248\n",
            "weighted avg       0.69      0.74      0.69     10248\n",
            "\n",
            "Acurácia: 0.7153411560191221\n",
            "[[3171  244]\n",
            " [1066  121]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.93      0.83      3415\n",
            "         1.0       0.33      0.10      0.16      1187\n",
            "\n",
            "    accuracy                           0.72      4602\n",
            "   macro avg       0.54      0.52      0.49      4602\n",
            "weighted avg       0.64      0.72      0.66      4602\n",
            "\n",
            "Acurácia: 0.7706489675516224\n",
            "[[4097  143]\n",
            " [1101   83]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.97      0.87      4240\n",
            "         1.0       0.37      0.07      0.12      1184\n",
            "\n",
            "    accuracy                           0.77      5424\n",
            "   macro avg       0.58      0.52      0.49      5424\n",
            "weighted avg       0.70      0.77      0.70      5424\n",
            "\n",
            "Acurácia: 0.7713829787234042\n",
            "[[7090  285]\n",
            " [1864  161]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.96      0.87      7375\n",
            "         1.0       0.36      0.08      0.13      2025\n",
            "\n",
            "    accuracy                           0.77      9400\n",
            "   macro avg       0.58      0.52      0.50      9400\n",
            "weighted avg       0.70      0.77      0.71      9400\n",
            "\n",
            "Acurácia: 0.7962529274004684\n",
            "[[3004   97]\n",
            " [ 686   56]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.97      0.88      3101\n",
            "         1.0       0.37      0.08      0.13       742\n",
            "\n",
            "    accuracy                           0.80      3843\n",
            "   macro avg       0.59      0.52      0.50      3843\n",
            "weighted avg       0.73      0.80      0.74      3843\n",
            "\n",
            "Acurácia: 0.7573781743308168\n",
            "[[4264  274]\n",
            " [1140  150]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.94      0.86      4538\n",
            "         1.0       0.35      0.12      0.18      1290\n",
            "\n",
            "    accuracy                           0.76      5828\n",
            "   macro avg       0.57      0.53      0.52      5828\n",
            "weighted avg       0.69      0.76      0.71      5828\n",
            "\n",
            "Modelos Clusters Treinados\n",
            "Calculou age\n",
            "Calculou upgrade\n",
            "Calculou gym status\n",
            "Calculou pref_modality\n",
            "Calculou num_distinct_gyms\n",
            "Calculou uses_per_week\n",
            "Calculou most_frequent_hour\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 407769/407769 [07:32<00:00, 900.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculou normalized_stats\n",
            "Calculou active_at_date\n",
            "2022-11-17 00:00:00.000000180\n",
            "Calculou churn_at_dt\n",
            "DF Temporal criado\n",
            "DF Inferencia criado\n",
            "Clusterizador criado\n",
            "Após o merge: (1764656, 32)\n",
            "Após remover duplicatas: (1136775, 32)\n",
            "DF Mergeado\n",
            "DF Preprocessado\n",
            "Acurácia: 0.6078923083095935\n",
            "[[18845  8116]\n",
            " [11429 11456]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.70      0.66     26961\n",
            "         1.0       0.59      0.50      0.54     22885\n",
            "\n",
            "    accuracy                           0.61     49846\n",
            "   macro avg       0.60      0.60      0.60     49846\n",
            "weighted avg       0.61      0.61      0.60     49846\n",
            "\n",
            "Modelo Geral Treinado\n",
            "Acurácia: 0.6368004960471245\n",
            "[[2882  902]\n",
            " [1441 1226]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.76      0.71      3784\n",
            "         1.0       0.58      0.46      0.51      2667\n",
            "\n",
            "    accuracy                           0.64      6451\n",
            "   macro avg       0.62      0.61      0.61      6451\n",
            "weighted avg       0.63      0.64      0.63      6451\n",
            "\n",
            "Acurácia: 0.6201282683769117\n",
            "[[1835  553]\n",
            " [ 987  679]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.77      0.70      2388\n",
            "         1.0       0.55      0.41      0.47      1666\n",
            "\n",
            "    accuracy                           0.62      4054\n",
            "   macro avg       0.60      0.59      0.59      4054\n",
            "weighted avg       0.61      0.62      0.61      4054\n",
            "\n",
            "Acurácia: 0.6034348165495707\n",
            "[[3021 1938]\n",
            " [2126 3163]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.61      0.60      4959\n",
            "         1.0       0.62      0.60      0.61      5289\n",
            "\n",
            "    accuracy                           0.60     10248\n",
            "   macro avg       0.60      0.60      0.60     10248\n",
            "weighted avg       0.60      0.60      0.60     10248\n",
            "\n",
            "Acurácia: 0.5893089960886571\n",
            "[[1614  841]\n",
            " [1049 1098]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.66      0.63      2455\n",
            "         1.0       0.57      0.51      0.54      2147\n",
            "\n",
            "    accuracy                           0.59      4602\n",
            "   macro avg       0.59      0.58      0.58      4602\n",
            "weighted avg       0.59      0.59      0.59      4602\n",
            "\n",
            "Acurácia: 0.5842551622418879\n",
            "[[1900  970]\n",
            " [1285 1269]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.66      0.63      2870\n",
            "         1.0       0.57      0.50      0.53      2554\n",
            "\n",
            "    accuracy                           0.58      5424\n",
            "   macro avg       0.58      0.58      0.58      5424\n",
            "weighted avg       0.58      0.58      0.58      5424\n",
            "\n",
            "Acurácia: 0.5996808510638297\n",
            "[[3187 1713]\n",
            " [2050 2450]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.65      0.63      4900\n",
            "         1.0       0.59      0.54      0.57      4500\n",
            "\n",
            "    accuracy                           0.60      9400\n",
            "   macro avg       0.60      0.60      0.60      9400\n",
            "weighted avg       0.60      0.60      0.60      9400\n",
            "\n",
            "Acurácia: 0.6044756700494406\n",
            "[[1563  593]\n",
            " [ 927  760]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.72      0.67      2156\n",
            "         1.0       0.56      0.45      0.50      1687\n",
            "\n",
            "    accuracy                           0.60      3843\n",
            "   macro avg       0.59      0.59      0.59      3843\n",
            "weighted avg       0.60      0.60      0.60      3843\n",
            "\n",
            "Acurácia: 0.6099862731640356\n",
            "[[2475  916]\n",
            " [1357 1080]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.73      0.69      3391\n",
            "         1.0       0.54      0.44      0.49      2437\n",
            "\n",
            "    accuracy                           0.61      5828\n",
            "   macro avg       0.59      0.59      0.59      5828\n",
            "weighted avg       0.60      0.61      0.60      5828\n",
            "\n",
            "Modelos Clusters Treinados\n",
            "Calculou age\n",
            "Calculou upgrade\n",
            "Calculou gym status\n",
            "Calculou pref_modality\n",
            "Calculou num_distinct_gyms\n",
            "Calculou uses_per_week\n",
            "Calculou most_frequent_hour\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 407769/407769 [07:33<00:00, 899.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculou normalized_stats\n",
            "Calculou active_at_date\n",
            "2022-11-17 00:00:00.000000270\n",
            "Calculou churn_at_dt\n",
            "DF Temporal criado\n",
            "DF Inferencia criado\n",
            "Clusterizador criado\n",
            "Após o merge: (1764656, 32)\n",
            "Após remover duplicatas: (1136775, 32)\n",
            "DF Mergeado\n",
            "DF Preprocessado\n",
            "Acurácia: 0.6216948200457408\n",
            "[[ 8890 11177]\n",
            " [ 7680 22099]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.54      0.44      0.49     20067\n",
            "         1.0       0.66      0.74      0.70     29779\n",
            "\n",
            "    accuracy                           0.62     49846\n",
            "   macro avg       0.60      0.59      0.59     49846\n",
            "weighted avg       0.61      0.62      0.61     49846\n",
            "\n",
            "Modelo Geral Treinado\n",
            "Acurácia: 0.5968066966361805\n",
            "[[1364 1477]\n",
            " [1124 2486]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.48      0.51      2841\n",
            "         1.0       0.63      0.69      0.66      3610\n",
            "\n",
            "    accuracy                           0.60      6451\n",
            "   macro avg       0.59      0.58      0.58      6451\n",
            "weighted avg       0.59      0.60      0.59      6451\n",
            "\n",
            "Acurácia: 0.5964479526393686\n",
            "[[1005  830]\n",
            " [ 806 1413]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.55      0.55      1835\n",
            "         1.0       0.63      0.64      0.63      2219\n",
            "\n",
            "    accuracy                           0.60      4054\n",
            "   macro avg       0.59      0.59      0.59      4054\n",
            "weighted avg       0.60      0.60      0.60      4054\n",
            "\n",
            "Acurácia: 0.664032006245121\n",
            "[[1234 2229]\n",
            " [1214 5571]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.36      0.42      3463\n",
            "         1.0       0.71      0.82      0.76      6785\n",
            "\n",
            "    accuracy                           0.66     10248\n",
            "   macro avg       0.61      0.59      0.59     10248\n",
            "weighted avg       0.64      0.66      0.65     10248\n",
            "\n",
            "Acurácia: 0.6003911342894394\n",
            "[[ 783 1052]\n",
            " [ 787 1980]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.43      0.46      1835\n",
            "         1.0       0.65      0.72      0.68      2767\n",
            "\n",
            "    accuracy                           0.60      4602\n",
            "   macro avg       0.58      0.57      0.57      4602\n",
            "weighted avg       0.59      0.60      0.59      4602\n",
            "\n",
            "Acurácia: 0.6196533923303835\n",
            "[[ 834 1278]\n",
            " [ 785 2527]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      0.39      0.45      2112\n",
            "         1.0       0.66      0.76      0.71      3312\n",
            "\n",
            "    accuracy                           0.62      5424\n",
            "   macro avg       0.59      0.58      0.58      5424\n",
            "weighted avg       0.61      0.62      0.61      5424\n",
            "\n",
            "Acurácia: 0.6385106382978724\n",
            "[[1567 2048]\n",
            " [1350 4435]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.54      0.43      0.48      3615\n",
            "         1.0       0.68      0.77      0.72      5785\n",
            "\n",
            "    accuracy                           0.64      9400\n",
            "   macro avg       0.61      0.60      0.60      9400\n",
            "weighted avg       0.63      0.64      0.63      9400\n",
            "\n",
            "Acurácia: 0.6229508196721312\n",
            "[[ 741  843]\n",
            " [ 606 1653]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.47      0.51      1584\n",
            "         1.0       0.66      0.73      0.70      2259\n",
            "\n",
            "    accuracy                           0.62      3843\n",
            "   macro avg       0.61      0.60      0.60      3843\n",
            "weighted avg       0.62      0.62      0.62      3843\n",
            "\n",
            "Acurácia: 0.6008922443376802\n",
            "[[1401 1233]\n",
            " [1093 2101]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      0.53      0.55      2634\n",
            "         1.0       0.63      0.66      0.64      3194\n",
            "\n",
            "    accuracy                           0.60      5828\n",
            "   macro avg       0.60      0.59      0.60      5828\n",
            "weighted avg       0.60      0.60      0.60      5828\n",
            "\n",
            "Modelos Clusters Treinados\n",
            "Calculou age\n",
            "Calculou upgrade\n",
            "Calculou gym status\n",
            "Calculou pref_modality\n",
            "Calculou num_distinct_gyms\n",
            "Calculou uses_per_week\n",
            "Calculou most_frequent_hour\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 407769/407769 [07:31<00:00, 903.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculou normalized_stats\n",
            "Calculou active_at_date\n",
            "2022-11-17 00:00:00.000000365\n",
            "Calculou churn_at_dt\n",
            "DF Temporal criado\n",
            "DF Inferencia criado\n",
            "Clusterizador criado\n",
            "Após o merge: (1764656, 32)\n",
            "Após remover duplicatas: (1136775, 32)\n",
            "DF Mergeado\n",
            "DF Preprocessado\n",
            "Acurácia: 0.7020623520442965\n",
            "[[ 4037 10591]\n",
            " [ 4260 30958]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.49      0.28      0.35     14628\n",
            "         1.0       0.75      0.88      0.81     35218\n",
            "\n",
            "    accuracy                           0.70     49846\n",
            "   macro avg       0.62      0.58      0.58     49846\n",
            "weighted avg       0.67      0.70      0.67     49846\n",
            "\n",
            "Modelo Geral Treinado\n",
            "Acurácia: 0.7195783599441947\n",
            "[[ 400 1410]\n",
            " [ 399 4242]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      0.22      0.31      1810\n",
            "         1.0       0.75      0.91      0.82      4641\n",
            "\n",
            "    accuracy                           0.72      6451\n",
            "   macro avg       0.63      0.57      0.57      6451\n",
            "weighted avg       0.68      0.72      0.68      6451\n",
            "\n",
            "Acurácia: 0.6425752343364578\n",
            "[[ 614  882]\n",
            " [ 567 1991]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      0.41      0.46      1496\n",
            "         1.0       0.69      0.78      0.73      2558\n",
            "\n",
            "    accuracy                           0.64      4054\n",
            "   macro avg       0.61      0.59      0.60      4054\n",
            "weighted avg       0.63      0.64      0.63      4054\n",
            "\n",
            "Acurácia: 0.7747853239656518\n",
            "[[ 439 1799]\n",
            " [ 509 7501]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.46      0.20      0.28      2238\n",
            "         1.0       0.81      0.94      0.87      8010\n",
            "\n",
            "    accuracy                           0.77     10248\n",
            "   macro avg       0.63      0.57      0.57     10248\n",
            "weighted avg       0.73      0.77      0.74     10248\n",
            "\n",
            "Acurácia: 0.665580182529335\n",
            "[[ 436 1033]\n",
            " [ 506 2627]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.46      0.30      0.36      1469\n",
            "         1.0       0.72      0.84      0.77      3133\n",
            "\n",
            "    accuracy                           0.67      4602\n",
            "   macro avg       0.59      0.57      0.57      4602\n",
            "weighted avg       0.64      0.67      0.64      4602\n",
            "\n",
            "Acurácia: 0.7144174041297935\n",
            "[[ 350 1111]\n",
            " [ 438 3525]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.44      0.24      0.31      1461\n",
            "         1.0       0.76      0.89      0.82      3963\n",
            "\n",
            "    accuracy                           0.71      5424\n",
            "   macro avg       0.60      0.56      0.57      5424\n",
            "weighted avg       0.68      0.71      0.68      5424\n",
            "\n",
            "Acurácia: 0.7154255319148937\n",
            "[[ 800 1821]\n",
            " [ 854 5925]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.48      0.31      0.37      2621\n",
            "         1.0       0.76      0.87      0.82      6779\n",
            "\n",
            "    accuracy                           0.72      9400\n",
            "   macro avg       0.62      0.59      0.60      9400\n",
            "weighted avg       0.69      0.72      0.69      9400\n",
            "\n",
            "Acurácia: 0.7140255009107468\n",
            "[[ 380  728]\n",
            " [ 371 2364]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.34      0.41      1108\n",
            "         1.0       0.76      0.86      0.81      2735\n",
            "\n",
            "    accuracy                           0.71      3843\n",
            "   macro avg       0.64      0.60      0.61      3843\n",
            "weighted avg       0.69      0.71      0.70      3843\n",
            "\n",
            "Acurácia: 0.6326355525051476\n",
            "[[ 888 1295]\n",
            " [ 846 2799]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.41      0.45      2183\n",
            "         1.0       0.68      0.77      0.72      3645\n",
            "\n",
            "    accuracy                           0.63      5828\n",
            "   macro avg       0.60      0.59      0.59      5828\n",
            "weighted avg       0.62      0.63      0.62      5828\n",
            "\n",
            "Modelos Clusters Treinados\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Executando a previsão\n",
        "df_final = previsao(dt_interval)\n",
        "display(df_final)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
